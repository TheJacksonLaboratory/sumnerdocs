{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HPC Sumner Documentation Work In Progress Documentation is in beta stage and not endorsed by Research IT at JAX. Before overhauling your existing setup, please read documentation, including external references. For any questions, you may post at HPC intranet forum and/or discuss in slack channel hpc-users-group . This documentation is inspired by similar websites: Sherlock at Stanford Yale Research Computing If you like to contribute to this documentation, feel free to send pull request by clicking edit icon on the top right corner of page.","title":"Home"},{"location":"conda/S01_conda/","text":"Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment. First-time Login \u00b6 ssh login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log2 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to /confs/dotfiles/hpc_default_env/ . ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), comment those out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and linuxbrew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to this one. Note that PATH variable will default to cent os 7 standard paths and LD_LIBRARY_PATH should preferably only contain entries related to slurm scheduler. exit #from sumner ## login again ssh login.sumner.jax.org Check common env variables set by default (HPC team) ## paths where all executables can be found echo $PATH /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/yoda/.local/bin:/home/yoda/bin ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 ## default loaded modules ## Most of paths in PATH, LD_LIBRARY_PATH, and LIBRARY_PATH are configured by these loaded modules. module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) dot 4) default-environment 5) slurm/18.08.8 6) gcc/8.2.0 Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a ~/bkup/confs/hpc_default_env/default_hpc_env.txt dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). module list For now, you may add following cmd to your ~/.bash_profile . Eventually it will go to ~/.profile.d/ setup detailed below. module unload dot For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload gcc first. module unload gcc module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 Unloading gcc Note that while starting pseudo-terminal using screen, tmux, or slurm interactive job, you may get module loaded gcc again in PATH. If so, make sure to do module unload gcc before running setup further. Backup and Reset Dotfiles \u00b6 Move dotfiles to archived directory If you have dotfiles and/or dot directories like .Renviron, .Rprofile, .curlrc, .cache, .config, etc., that may cause issues configuring environment. Moving dotfiles Following is what I've done but may not be safe unless you know what you are doing! Moving some of dotfiles is tricky as they are required for login to sumner. If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. ## WARN: Moving all files and directories starting ## with dot to archived dir. mkdir -p \" ${ HOME } \" /legacy_env && \\ mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ ## DO NOT FORGET TO COPY BACK following files to ## \"${HOME}\"/ else you may get locked out of sumner. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.bash* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ rsync -avhP legacy_env/.parallel ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ ## make empty dirs ## note that user .local and .config, if any are now backed up and ## we are creating a empty ~/.local directory mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## CONFIRM FROM A SEPARATE TERMINAL that you can login to sumner ssh yoda@sumner env If above command succeeds and env looks similar (PATH in particular) to PATH and LD_LIBRARY_PATH shown above, you're good! You can exit old sumner session and install anaconda3 in new terminal session. Start Interactive Job prior to installation Prefer running setup on a dedicated interactive node instead of login node. Some of conda install/update steps may get killed on login node. srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Install anaconda3 \u00b6 Python3 over Python2 Prefer using Anaconda3 (Python3) over Python2 as the latter has reached End of Life . For anaconda3: Using v2019.03-Linux-x86_64 with permalink and MD5 : b77a71c3712b45c8f33c7b2ecade366c. cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ md5sum Anaconda3-2019.10-Linux-x86_64.sh Setup anaconda3 environment with default options, i.e., install at ~/anaconda3 . cd \" $HOME \" && \\ bash ~/Downloads/conda/Anaconda3-2019.10-Linux-x86_64.sh Once conda installation is complete at default location, logout and login to sumner. Default conda env, base will now be in effect. Note (base) prefix to your username. Also, check output of following: echo $CONDA_DEFAULT_ENV base echo $CONDA_PREFIX /home/yoda/anaconda3 Configuring Conda \u00b6 Following are additional configuration within conda environment to enable installation of R 3.6.1+ and Jupyter. Set channel priority \u00b6 Important Prefer installing compilers only from a single channel and avoid mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled at default for conda v4.6 or higher. Add Bioconda and conda-forge channels to get updated and compbio related packages. Avoid changing order of following commands unless you prefer to keep a different priority for these channels. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and set priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as shown below. This file should be present after above commands and no need to edit unless changing priority of channels. This is in yaml format file. Please take care of preceding spaces (and not tabs) before and after - while editing this file. channels: - conda-forge - bioconda - defaults Install R 3.6.1 \u00b6 For compatibility with conda env, prefer installing R 3.6 via conda. Prefer installing R env from conda-forge channel which is already a priority channel in our ~/.condarc . We also specify R version, just to make sure that we install R v3.6.1. Beware of non-standard conda packages Note that most up-to-date R version may be available in the same or other conda channels, like r or bioconda but it is preferable to install R from the first priority channel, i.e., conda-forge in our case, and where package does not show non-standard labels at anaconda website, e.g., As of writing this section, R 3.6.2 on anaconda website shows use of non-standard labels, most likely because it was updated a few days before and so, may not comply with all of conda dependency. ## look for a line r-base and check source channel. ## If it is other than conda-forge, try to downgrade R package where ## r-base is available under conda-forge channel. conda install -c conda-forge r-base = 3 .6.1 Above command will take 15-30 min to resolve dependencies. It will do major overhaul of default conda environment by... upgrading conda to latest version, 4.8.2 or higher. installing conda and r-base from conda-forge source. and much more... Pin R and conda auto-updates \u00b6 Before moving further, let's pin R version to 3.6.1 and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do conda install <pkg> in future, and carefully install/update packages without breaking existing setup. Compile over conda install Rpkg Typically, I avoid installing or updating package if conda install throws a message or warning about removing or downgrading existing packages. In such cases, I fall back to compiling package using available devtools in conda and sumner. Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. conda config --set auto_update_conda False ## add following to pinned file nano ~/anaconda3/conda-meta/pinned r-base ==3.6.1 Setup Rprofile and Renviron \u00b6 Setup R package directory path for R 3.6 mkdir -p ~/R/pkgs/v3.6 Create these two files to setup startup env for R Renviron nano ~/.Renviron Where should R save new packages? Over time, this directory may grow in size, so better to keep under tier1 storage. R_LIBS = \"/projects/verhaak-lab/yoda/sumnerenv_os7/R/pkgs/v3.6:/home/yoda/R/pkgs/v3.6:/home/yoda/anaconda3/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() command. Note that /home/yoda/anaconda3/lib/R/library is a required path set while installing R using conda. R profile nano ~/.Rprofile ## set user specific env variables, if any here ## e.g., GITHUB_PAT if here Sys.setenv(\"GITHUB_PAT\"=\"xyzabc1234\") ## Default source to download packages local({ r <- getOption(\"repos\") r[\"CRAN\"] <- \"https://cran.rstudio.com\" options(repos = r) }) Before further configuring sumner env, let's logout and login first from interactive job and exit sumner. Then, login back to sumner and start interactive queue again. exit # from interactive session exit # from sumner ssh sumner ## start interactive session srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Confirm that login env is similar to earlier env (just after anaconda3 setup) by running env command. PATH should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc. Also, make sure that module: gcc is unloaded. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools. ## unload gcc if loaded module unload gcc ## example PATH variable /home/yoda/anaconda3/bin:/home/yoda/anaconda3/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin :/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/yoda/.local/bin:/home/yoda/bin Avoid Rpkgs using conda install Install essential R packages and jupyter kernel for R only if they are available for R 3.6.x (preferably v3.6.1) from conda-forge channel. This may not be a case for most times, e.g., for R v3.5.1, I can find compatible packages but not for R 3.6. I avoid downloading r-essentials from other channels, including r as some packages have dependencies on shared library which are different between r and conda-forge channel. I prefer installing R packages using install.packages , devtools::install_github() or BiocManager command in R. ## This did not work for R 3.6.1 # conda install -c conda-forge r-essentials Installing R and linux packages \u00b6 Here, I alternate between R and bash to install packages I use on regular basis. It is going to take a while to install these packages. You may not need to install all packages but if you are skipping ahead and get into error because of missing library, package, etc., you probably need to revisit this code block and confirm that you have all of dependencies in your environment for successful installation. install.packages ( \"devtools\" ) When you start installing R packages, notice x86_64-conda_cos6-linux-gnu-cc or other compilers that R is using instead of /usr/bin/gcc or sumner-defaults. Similarly, during loading R packages which requires shared libraries, we link shared libs from conda and not from system-defaults (gcc and others). That's one of reasons I did module unload gcc before installing R packages. Eventually, we will set ~/.profile.d environment such that it will always load conda environment and ignore gcc and other devtools from system paths. Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment. Backup conda env \u00b6 Base environment. ## script is on github repo under conds/bin/ ./conda_bkup.sh","title":"Setup"},{"location":"conda/S01_conda/#first-time-login","text":"ssh login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log2 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to /confs/dotfiles/hpc_default_env/ . ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), comment those out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and linuxbrew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to this one. Note that PATH variable will default to cent os 7 standard paths and LD_LIBRARY_PATH should preferably only contain entries related to slurm scheduler. exit #from sumner ## login again ssh login.sumner.jax.org Check common env variables set by default (HPC team) ## paths where all executables can be found echo $PATH /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/yoda/.local/bin:/home/yoda/bin ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 ## default loaded modules ## Most of paths in PATH, LD_LIBRARY_PATH, and LIBRARY_PATH are configured by these loaded modules. module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) dot 4) default-environment 5) slurm/18.08.8 6) gcc/8.2.0 Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a ~/bkup/confs/hpc_default_env/default_hpc_env.txt dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). module list For now, you may add following cmd to your ~/.bash_profile . Eventually it will go to ~/.profile.d/ setup detailed below. module unload dot For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload gcc first. module unload gcc module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 Unloading gcc Note that while starting pseudo-terminal using screen, tmux, or slurm interactive job, you may get module loaded gcc again in PATH. If so, make sure to do module unload gcc before running setup further.","title":"First-time Login"},{"location":"conda/S01_conda/#backup-and-reset-dotfiles","text":"Move dotfiles to archived directory If you have dotfiles and/or dot directories like .Renviron, .Rprofile, .curlrc, .cache, .config, etc., that may cause issues configuring environment. Moving dotfiles Following is what I've done but may not be safe unless you know what you are doing! Moving some of dotfiles is tricky as they are required for login to sumner. If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. ## WARN: Moving all files and directories starting ## with dot to archived dir. mkdir -p \" ${ HOME } \" /legacy_env && \\ mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ ## DO NOT FORGET TO COPY BACK following files to ## \"${HOME}\"/ else you may get locked out of sumner. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.bash* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ rsync -avhP legacy_env/.parallel ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ ## make empty dirs ## note that user .local and .config, if any are now backed up and ## we are creating a empty ~/.local directory mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## CONFIRM FROM A SEPARATE TERMINAL that you can login to sumner ssh yoda@sumner env If above command succeeds and env looks similar (PATH in particular) to PATH and LD_LIBRARY_PATH shown above, you're good! You can exit old sumner session and install anaconda3 in new terminal session. Start Interactive Job prior to installation Prefer running setup on a dedicated interactive node instead of login node. Some of conda install/update steps may get killed on login node. srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash","title":"Backup and Reset Dotfiles"},{"location":"conda/S01_conda/#install-anaconda3","text":"Python3 over Python2 Prefer using Anaconda3 (Python3) over Python2 as the latter has reached End of Life . For anaconda3: Using v2019.03-Linux-x86_64 with permalink and MD5 : b77a71c3712b45c8f33c7b2ecade366c. cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ md5sum Anaconda3-2019.10-Linux-x86_64.sh Setup anaconda3 environment with default options, i.e., install at ~/anaconda3 . cd \" $HOME \" && \\ bash ~/Downloads/conda/Anaconda3-2019.10-Linux-x86_64.sh Once conda installation is complete at default location, logout and login to sumner. Default conda env, base will now be in effect. Note (base) prefix to your username. Also, check output of following: echo $CONDA_DEFAULT_ENV base echo $CONDA_PREFIX /home/yoda/anaconda3","title":"Install anaconda3"},{"location":"conda/S01_conda/#configuring-conda","text":"Following are additional configuration within conda environment to enable installation of R 3.6.1+ and Jupyter.","title":"Configuring Conda"},{"location":"conda/S01_conda/#set-channel-priority","text":"Important Prefer installing compilers only from a single channel and avoid mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled at default for conda v4.6 or higher. Add Bioconda and conda-forge channels to get updated and compbio related packages. Avoid changing order of following commands unless you prefer to keep a different priority for these channels. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and set priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as shown below. This file should be present after above commands and no need to edit unless changing priority of channels. This is in yaml format file. Please take care of preceding spaces (and not tabs) before and after - while editing this file. channels: - conda-forge - bioconda - defaults","title":"Set channel priority"},{"location":"conda/S01_conda/#install-r-361","text":"For compatibility with conda env, prefer installing R 3.6 via conda. Prefer installing R env from conda-forge channel which is already a priority channel in our ~/.condarc . We also specify R version, just to make sure that we install R v3.6.1. Beware of non-standard conda packages Note that most up-to-date R version may be available in the same or other conda channels, like r or bioconda but it is preferable to install R from the first priority channel, i.e., conda-forge in our case, and where package does not show non-standard labels at anaconda website, e.g., As of writing this section, R 3.6.2 on anaconda website shows use of non-standard labels, most likely because it was updated a few days before and so, may not comply with all of conda dependency. ## look for a line r-base and check source channel. ## If it is other than conda-forge, try to downgrade R package where ## r-base is available under conda-forge channel. conda install -c conda-forge r-base = 3 .6.1 Above command will take 15-30 min to resolve dependencies. It will do major overhaul of default conda environment by... upgrading conda to latest version, 4.8.2 or higher. installing conda and r-base from conda-forge source. and much more...","title":"Install R 3.6.1"},{"location":"conda/S01_conda/#pin-r-and-conda-auto-updates","text":"Before moving further, let's pin R version to 3.6.1 and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do conda install <pkg> in future, and carefully install/update packages without breaking existing setup. Compile over conda install Rpkg Typically, I avoid installing or updating package if conda install throws a message or warning about removing or downgrading existing packages. In such cases, I fall back to compiling package using available devtools in conda and sumner. Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. conda config --set auto_update_conda False ## add following to pinned file nano ~/anaconda3/conda-meta/pinned r-base ==3.6.1","title":"Pin R and conda auto-updates"},{"location":"conda/S01_conda/#setup-rprofile-and-renviron","text":"Setup R package directory path for R 3.6 mkdir -p ~/R/pkgs/v3.6 Create these two files to setup startup env for R Renviron nano ~/.Renviron Where should R save new packages? Over time, this directory may grow in size, so better to keep under tier1 storage. R_LIBS = \"/projects/verhaak-lab/yoda/sumnerenv_os7/R/pkgs/v3.6:/home/yoda/R/pkgs/v3.6:/home/yoda/anaconda3/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() command. Note that /home/yoda/anaconda3/lib/R/library is a required path set while installing R using conda. R profile nano ~/.Rprofile ## set user specific env variables, if any here ## e.g., GITHUB_PAT if here Sys.setenv(\"GITHUB_PAT\"=\"xyzabc1234\") ## Default source to download packages local({ r <- getOption(\"repos\") r[\"CRAN\"] <- \"https://cran.rstudio.com\" options(repos = r) }) Before further configuring sumner env, let's logout and login first from interactive job and exit sumner. Then, login back to sumner and start interactive queue again. exit # from interactive session exit # from sumner ssh sumner ## start interactive session srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Confirm that login env is similar to earlier env (just after anaconda3 setup) by running env command. PATH should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc. Also, make sure that module: gcc is unloaded. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools. ## unload gcc if loaded module unload gcc ## example PATH variable /home/yoda/anaconda3/bin:/home/yoda/anaconda3/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin :/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/yoda/.local/bin:/home/yoda/bin Avoid Rpkgs using conda install Install essential R packages and jupyter kernel for R only if they are available for R 3.6.x (preferably v3.6.1) from conda-forge channel. This may not be a case for most times, e.g., for R v3.5.1, I can find compatible packages but not for R 3.6. I avoid downloading r-essentials from other channels, including r as some packages have dependencies on shared library which are different between r and conda-forge channel. I prefer installing R packages using install.packages , devtools::install_github() or BiocManager command in R. ## This did not work for R 3.6.1 # conda install -c conda-forge r-essentials","title":"Setup Rprofile and Renviron"},{"location":"conda/S01_conda/#installing-r-and-linux-packages","text":"Here, I alternate between R and bash to install packages I use on regular basis. It is going to take a while to install these packages. You may not need to install all packages but if you are skipping ahead and get into error because of missing library, package, etc., you probably need to revisit this code block and confirm that you have all of dependencies in your environment for successful installation. install.packages ( \"devtools\" ) When you start installing R packages, notice x86_64-conda_cos6-linux-gnu-cc or other compilers that R is using instead of /usr/bin/gcc or sumner-defaults. Similarly, during loading R packages which requires shared libraries, we link shared libs from conda and not from system-defaults (gcc and others). That's one of reasons I did module unload gcc before installing R packages. Eventually, we will set ~/.profile.d environment such that it will always load conda environment and ignore gcc and other devtools from system paths. Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment.","title":"Installing R and linux packages"},{"location":"conda/S01_conda/#backup-conda-env","text":"Base environment. ## script is on github repo under conds/bin/ ./conda_bkup.sh","title":"Backup conda env"},{"location":"conda/S02_gpu_config/","text":"TensorFlow v2 \u00b6 Run interactive session on gpu compute node. GPU nodes starts with hostname winter while cpu nodes have prefix, sumner . cd /fastscratch/ \" $USER \" ## requesting partition: gpu with gpu cores of 2 srun --job-name = tflow_gpu --partition = gpu --qos = batch --time = 04 :00:00 --mem = 8G --nodes = 1 --ntasks = 2 --mail-type = FAIL --export = all--pty --gres gpu:2 bash --login Load GPU drivers. TensorFlow requires CUDA-enabled GPU cards. Read detailed specifications under Hardware and Software requirements. Ask Research IT for exact version GPU drivers, typically from NVIDIA are hardware-specific and not all GPU drivers may work with all hardware versions. Check with Research IT to make sure that required software are installed on gpu nodes as some may need root privileges. Using CUDA 10.1.243 module load cuda10.1/toolkit/10.1.243 ## CUDA_HOME echo \" $CUDA_HOME \" ## Check CUDA driver version cat \" $CUDA_HOME \" /version.txt ## or nvcc --version ## For cuDNN ## Ask Research IT if cuDNN library is not installed for a specific CUDA driver cat \" $CUDA_HOME \" /include/cudnn.h | grep CUDNN_MAJOR -A 2 ## Check GPU card info: Available GPU cores, version, etc. nvidia-smi /cm/shared/apps/cuda10.1/toolkit/10.1.243 CUDA release 10.1, V10.1.243 cuDNN v7.6.5.32 NVIDIA-SMI 418.87.01 | Driver Version: 418.87.01 | CUDA Version: 10.1 If you do not see cuDNN SDK installed, either contact Research IT or install it by yourself. The latter option requires familiarity with Modulefiles as you may need to clone HPC module and libraries on your end, and then install cuDNN using this guide or this SO post . Note that guide may be outdated for cuDNN version, and make sure that cuDNN version is aligned to CUDA driver version, i.e., 10.1, installed on gpu nodes. cd ~/tmp tar xvzf cudnn-10.1-linux-x64-v7.6.5.32.tgz cp cuda/include/cudnn.h <USER_INSTALLED_PATH>/cuda/include cp cuda/lib64/libcudnn* <USER_INSTALLED_PATH>/cuda/lib64 chmod a+r <USER_INSTALLED_PATH>/include/cudnn.h <USER_INSTALLED_PATH>/cuda/lib64/libcudnn* To install, follow official guide from anaconda team. Prefer creating a dedicated env for tensorflow2 gpu Prefer using stable tensorflow over nightly builds. Prefer installing CPU vs. GPU versions in separate and new conda env. Avoid loading system modules for CUDA toolkit or other dependencies unless documented in installation guide above. ## By default, tf now used tf2 over tf1 ## Using v2.1.0 conda create -n tf-gpu tensorflow-gpu conda activate tf-gpu Verify install Check GPU usage While running tutorials, you may run watch nvidia-smi in a separate tmux/screen pane to confirm that gpus and not cpus are being used for computing. check tensorflow version python -c 'import tensorflow as tf; print(tf.__version__)' v2.1.0 or higher test run python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" Should show Tensor(\"Sum:0\", shape=(), dtype=float32) . For additional tutorials, checkout official guide . Tensorflow Jupyter Kernel \u00b6 Install separate kernel for tensorflow jupyter env. conda activate tf-gpu conda install ipykernel python -m ipykernel install --user --name tf-gpu --display-name \"tf-gpu\" This will install a separate python3 kernel at .local/share/jupyter/kernels/tf-gpu . So, when you start jupyter notebook even from a default (base) env, tf-gpu kernel will show up besides defauly Python 3 kernel. However, you need to be on compatible GPU compute node to make use of tf-gpu kernel. Load valid conda env before loading kernel While we have installed tf-gpu kernel, it usually loads default or base conda environment and not tf-gpu conda env that we created just above. This may land you into issues as PATH and other variables from base env can conflict with conda tf-gpu env. You can override environment prior to loading custom kernel, like tf-gpu using trick from @minrk First make a loading script, say tf-gpu-env and place it somewhere outside jupyter kernel dir. mkdir -p ~/bin/kernels nano ~/bin/kernels/tf-gpu-env Edit tf-gpu-env to load required environment. The last command exec... will execute tf-gpu kernel and will inherit your custom loaded environment in jupyter notebook. environment variables in notebook Note that all of environment variables, including API tokens, session cookies, etc. if present in environment will be exposed to jupyter notebook. You may unset those in the script below prior to exec... command. #!/bin/bash ## Load env before loading jupyter kernel ## @sbamin ## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE = $( conda info --base ) && \\ source \" ${ CONDA_BASE } \" /etc/profile.d/conda.sh && \\ conda activate tf-gpu #### END CONDA SETUP #### ## Load CUDA toolkit module load s7cuda/toolkit/10.1.243 # this is the critical part, and should be at the end of your script: exec /home/amins/anaconda3/envs/tf-gpu/bin/python -m ipykernel_launcher \" $@ \" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/tf-gpu/kernel.json #_end_ Finally, backup kernel.json under ~/.local/share/jupyter/kernels/tf-gpu/ and replace it with following. { \"argv\" : [ \"/home/amins/bin/kernels/tf-gpu-env\" , \"-f\" , \"{connection_file}\" ], \"display_name\" : \"tf-gpu\" , \"language\" : \"python\" } Reload tf-gpu kernel in notebook and run something like !env . You will notice a valid tf-gpu conda env instead of default conda base environment. Tensorboard \u00b6 Tensorboard typically comes installed with TensorFlow but conda version of tensorflow (v2.1.0) is shipping with tensorboard 2.1.1 in python 3.8 and not within the conda default python 3.7. This may create problems loading tensorboard as an extension within jupyter notebook. As an interim fix, I've reinstalled tensorboard using pip . Error related to opt-einsum package If you get following error but still have tensorboard installed, this should be harmless as long as you have valid opt-einsum package. ERROR: tensorflow 2.1.0 has requirement opt-einsum>=2.3.2, but you'll have opt-einsum 0+untagged.56.g2664021.dirty which is incompatible. You can check using conda list | grep -E \"opt-einsum\" or using pip freeze | grep -E \"opt-einsum\" . I already had opt-einsum v3.2.1 but error was likely due to crytpic version name from conda repo. conda activate tf-gpu pip install tensorboard == 2 .1.1 python -c 'import tensorflow as tf; print(tf.__version__)' python -c 'import tensorboard as tb; print(tb.__version__)' TensorFlow: 2.1.0 TensorBoard: 2.1.1 Tensorboard: Getting started guide Tensorboard server over notebook extension Instead of using tensorboard notebook extension, prefer to run tensorboard as a standalone server, e.g., conda activate tf-gpu && \\ cd <workdir> && \\ tensorboard serve --logdir logs --bind_all TensorRT \u00b6 Optional install for backward compatibility Visit TensorRT install page for detailed guide. Make sure to follow requirements given under Getting Started section, esp. PyCUDA. Download tarball specific to CUDA and cuDNN version, e.g., CUDA 10.1 and cuDNN 7.6 as above. Accordingly, I have downloaded TensorRT-6.0.1.5.CentOS-7.6.x86_64-gnu.cuda-10.1.cudnn7.6.tar.gz . Follow instructions for tar file base installation. Know that CUDA and cuDNN versions in default instructions may be misleading and you should instead download TensorRT version based on actual CUDA and cuDNN version installed on your system. Extract tarball conda activate tf-gpu ## Make sure to load CUDA libs, including cuDNN libs module load s7cuda/toolkit/10.1.243 cd /projects/verhaak-lab/amins/sumnerenv_os7/opt/gpu/tensorRT tar xvzf TensorRT-6.0.1.5.CentOS-7.6.x86_64-gnu.cuda-10.1.cudnn7.6.tar.gz cd TensorRT-6.0.1.5 Temporarily export lib path to LD_LIBRARY_PATH. After installation, lib path will rather be put in Modulefile. LD_LIBRARY_PATH = \" $LD_LIBRARY_PATH : $SUM7ENV /opt/gpu/tensorRT/TensorRT-6.0.1.5/lib\" export LD_LIBRARY_PATH Install the Python TensorRT wheel file cd python && \\ pip install tensorrt-6.0.1.5-cp37-none-linux_x86_64.whl |& install.log 1 2 3 Processing ./tensorrt-6.0.1.5-cp37-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-6.0.1.5 Install the Python UFF wheel file: Required for working with TensorFlow2 cd ../uff && \\ pip install uff-0.6.5-py2.py3-none-any.whl |& tee -a install.log && \\ command -v convert-to-uff 1 2 3 4 5 6 7 Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from uff==0.6.5) (1.18.1) Requirement already satisfied: protobuf>=3.3.0 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from uff==0.6.5) (3.11.4) Requirement already satisfied: six>=1.9 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.14.0) Requirement already satisfied: setuptools in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from protobuf>=3.3.0->uff==0.6.5) (46.1.3.post20200325) Installing collected packages: uff Successfully installed uff-0.6.5 Install the Python graphsurgeon wheel file cd ../graphsurgeon && \\ pip install graphsurgeon-0.4.1-py2.py3-none-any.whl |& tee -a install.log 1 2 3 Processing ./graphsurgeon-0.4.1-py2.py3-none-any.whl Installing collected packages: graphsurgeon Successfully installed graphsurgeon-0.4.1 Test compilation and Hello Word example cd ../samples/sampleMNIST && \\ make && \\ cd ../../data/mnist && \\ ## Download MNIST dataset wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz && \\ ls *ubyte.gz | parallel -j2 gunzip {} ./generate_pgms.py && \\ cd ../.. && \\ ./bin/sample_mnist -h && \\ ./bin/sample_mnist --datadir = data/mnist Output shows predicted probability of ASCII formatted input number. After successful install, update Modulefile, s7cuda/toolkit/10.1.243 by adding TensorRT env configs. # TensorRT set tenrthome / projects / verhaak-lab / amins / sumnerenv_os7 / opt / gpu / tensorRT / TensorRT-6.0.1.5 setenv TENSORRT_HOME $tenrthome append - path LD_LIBRARY_PATH $tenrthome / lib append - path PATH $tenrthome / bin Restart terminal and test again conda activate tf-gpu && \\ module load s7cuda/toolkit/10.1.243 && \\ ## sample_mnist will be in PATH sample_mnist --datadir = \" $TENSORRT_HOME \" /data/mnist PyTorch \u00b6 Using PyTorch stable build v1.5 for CUDA 10.1, and installing pytorch-1.4.0-py3.7_cuda10.1.243_cudnn7.6.3_0 conda package along with dependencies. Using conda installed CUDA toolkit Unlike tensorflow installation which depends on module load cuda10.1/toolkit/10.1.243 , PyTorch installation will also install CUDA toolkit v10.1. Note that toolkit version must be compatible with GPU card driver, i.e., toolkit v10.1 is compatible with GPU card driver Version: 418.87.01. You can check driver and toolkit compatibility from NVIDIA Drivers Download page. conda activate tf-gpu ## I'd installed matplotlib for other purpose prior to installing pytorch conda activate matplotlib ## Get updated install cmd from https://pytorch.org conda install pytorch torchvision cudatoolkit = 10 .1 -c pytorch Verify installation from __future__ import print_function import torch x = torch . rand ( 5 , 3 ) print ( x ) Above will output a tensor of 5x3 dimensions.","title":"GPU config"},{"location":"conda/S02_gpu_config/#tensorflow-v2","text":"Run interactive session on gpu compute node. GPU nodes starts with hostname winter while cpu nodes have prefix, sumner . cd /fastscratch/ \" $USER \" ## requesting partition: gpu with gpu cores of 2 srun --job-name = tflow_gpu --partition = gpu --qos = batch --time = 04 :00:00 --mem = 8G --nodes = 1 --ntasks = 2 --mail-type = FAIL --export = all--pty --gres gpu:2 bash --login Load GPU drivers. TensorFlow requires CUDA-enabled GPU cards. Read detailed specifications under Hardware and Software requirements. Ask Research IT for exact version GPU drivers, typically from NVIDIA are hardware-specific and not all GPU drivers may work with all hardware versions. Check with Research IT to make sure that required software are installed on gpu nodes as some may need root privileges. Using CUDA 10.1.243 module load cuda10.1/toolkit/10.1.243 ## CUDA_HOME echo \" $CUDA_HOME \" ## Check CUDA driver version cat \" $CUDA_HOME \" /version.txt ## or nvcc --version ## For cuDNN ## Ask Research IT if cuDNN library is not installed for a specific CUDA driver cat \" $CUDA_HOME \" /include/cudnn.h | grep CUDNN_MAJOR -A 2 ## Check GPU card info: Available GPU cores, version, etc. nvidia-smi /cm/shared/apps/cuda10.1/toolkit/10.1.243 CUDA release 10.1, V10.1.243 cuDNN v7.6.5.32 NVIDIA-SMI 418.87.01 | Driver Version: 418.87.01 | CUDA Version: 10.1 If you do not see cuDNN SDK installed, either contact Research IT or install it by yourself. The latter option requires familiarity with Modulefiles as you may need to clone HPC module and libraries on your end, and then install cuDNN using this guide or this SO post . Note that guide may be outdated for cuDNN version, and make sure that cuDNN version is aligned to CUDA driver version, i.e., 10.1, installed on gpu nodes. cd ~/tmp tar xvzf cudnn-10.1-linux-x64-v7.6.5.32.tgz cp cuda/include/cudnn.h <USER_INSTALLED_PATH>/cuda/include cp cuda/lib64/libcudnn* <USER_INSTALLED_PATH>/cuda/lib64 chmod a+r <USER_INSTALLED_PATH>/include/cudnn.h <USER_INSTALLED_PATH>/cuda/lib64/libcudnn* To install, follow official guide from anaconda team. Prefer creating a dedicated env for tensorflow2 gpu Prefer using stable tensorflow over nightly builds. Prefer installing CPU vs. GPU versions in separate and new conda env. Avoid loading system modules for CUDA toolkit or other dependencies unless documented in installation guide above. ## By default, tf now used tf2 over tf1 ## Using v2.1.0 conda create -n tf-gpu tensorflow-gpu conda activate tf-gpu Verify install Check GPU usage While running tutorials, you may run watch nvidia-smi in a separate tmux/screen pane to confirm that gpus and not cpus are being used for computing. check tensorflow version python -c 'import tensorflow as tf; print(tf.__version__)' v2.1.0 or higher test run python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" Should show Tensor(\"Sum:0\", shape=(), dtype=float32) . For additional tutorials, checkout official guide .","title":"TensorFlow v2"},{"location":"conda/S02_gpu_config/#tensorflow-jupyter-kernel","text":"Install separate kernel for tensorflow jupyter env. conda activate tf-gpu conda install ipykernel python -m ipykernel install --user --name tf-gpu --display-name \"tf-gpu\" This will install a separate python3 kernel at .local/share/jupyter/kernels/tf-gpu . So, when you start jupyter notebook even from a default (base) env, tf-gpu kernel will show up besides defauly Python 3 kernel. However, you need to be on compatible GPU compute node to make use of tf-gpu kernel. Load valid conda env before loading kernel While we have installed tf-gpu kernel, it usually loads default or base conda environment and not tf-gpu conda env that we created just above. This may land you into issues as PATH and other variables from base env can conflict with conda tf-gpu env. You can override environment prior to loading custom kernel, like tf-gpu using trick from @minrk First make a loading script, say tf-gpu-env and place it somewhere outside jupyter kernel dir. mkdir -p ~/bin/kernels nano ~/bin/kernels/tf-gpu-env Edit tf-gpu-env to load required environment. The last command exec... will execute tf-gpu kernel and will inherit your custom loaded environment in jupyter notebook. environment variables in notebook Note that all of environment variables, including API tokens, session cookies, etc. if present in environment will be exposed to jupyter notebook. You may unset those in the script below prior to exec... command. #!/bin/bash ## Load env before loading jupyter kernel ## @sbamin ## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE = $( conda info --base ) && \\ source \" ${ CONDA_BASE } \" /etc/profile.d/conda.sh && \\ conda activate tf-gpu #### END CONDA SETUP #### ## Load CUDA toolkit module load s7cuda/toolkit/10.1.243 # this is the critical part, and should be at the end of your script: exec /home/amins/anaconda3/envs/tf-gpu/bin/python -m ipykernel_launcher \" $@ \" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/tf-gpu/kernel.json #_end_ Finally, backup kernel.json under ~/.local/share/jupyter/kernels/tf-gpu/ and replace it with following. { \"argv\" : [ \"/home/amins/bin/kernels/tf-gpu-env\" , \"-f\" , \"{connection_file}\" ], \"display_name\" : \"tf-gpu\" , \"language\" : \"python\" } Reload tf-gpu kernel in notebook and run something like !env . You will notice a valid tf-gpu conda env instead of default conda base environment.","title":"Tensorflow Jupyter Kernel"},{"location":"conda/S02_gpu_config/#tensorboard","text":"Tensorboard typically comes installed with TensorFlow but conda version of tensorflow (v2.1.0) is shipping with tensorboard 2.1.1 in python 3.8 and not within the conda default python 3.7. This may create problems loading tensorboard as an extension within jupyter notebook. As an interim fix, I've reinstalled tensorboard using pip . Error related to opt-einsum package If you get following error but still have tensorboard installed, this should be harmless as long as you have valid opt-einsum package. ERROR: tensorflow 2.1.0 has requirement opt-einsum>=2.3.2, but you'll have opt-einsum 0+untagged.56.g2664021.dirty which is incompatible. You can check using conda list | grep -E \"opt-einsum\" or using pip freeze | grep -E \"opt-einsum\" . I already had opt-einsum v3.2.1 but error was likely due to crytpic version name from conda repo. conda activate tf-gpu pip install tensorboard == 2 .1.1 python -c 'import tensorflow as tf; print(tf.__version__)' python -c 'import tensorboard as tb; print(tb.__version__)' TensorFlow: 2.1.0 TensorBoard: 2.1.1 Tensorboard: Getting started guide Tensorboard server over notebook extension Instead of using tensorboard notebook extension, prefer to run tensorboard as a standalone server, e.g., conda activate tf-gpu && \\ cd <workdir> && \\ tensorboard serve --logdir logs --bind_all","title":"Tensorboard"},{"location":"conda/S02_gpu_config/#tensorrt","text":"Optional install for backward compatibility Visit TensorRT install page for detailed guide. Make sure to follow requirements given under Getting Started section, esp. PyCUDA. Download tarball specific to CUDA and cuDNN version, e.g., CUDA 10.1 and cuDNN 7.6 as above. Accordingly, I have downloaded TensorRT-6.0.1.5.CentOS-7.6.x86_64-gnu.cuda-10.1.cudnn7.6.tar.gz . Follow instructions for tar file base installation. Know that CUDA and cuDNN versions in default instructions may be misleading and you should instead download TensorRT version based on actual CUDA and cuDNN version installed on your system. Extract tarball conda activate tf-gpu ## Make sure to load CUDA libs, including cuDNN libs module load s7cuda/toolkit/10.1.243 cd /projects/verhaak-lab/amins/sumnerenv_os7/opt/gpu/tensorRT tar xvzf TensorRT-6.0.1.5.CentOS-7.6.x86_64-gnu.cuda-10.1.cudnn7.6.tar.gz cd TensorRT-6.0.1.5 Temporarily export lib path to LD_LIBRARY_PATH. After installation, lib path will rather be put in Modulefile. LD_LIBRARY_PATH = \" $LD_LIBRARY_PATH : $SUM7ENV /opt/gpu/tensorRT/TensorRT-6.0.1.5/lib\" export LD_LIBRARY_PATH Install the Python TensorRT wheel file cd python && \\ pip install tensorrt-6.0.1.5-cp37-none-linux_x86_64.whl |& install.log 1 2 3 Processing ./tensorrt-6.0.1.5-cp37-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-6.0.1.5 Install the Python UFF wheel file: Required for working with TensorFlow2 cd ../uff && \\ pip install uff-0.6.5-py2.py3-none-any.whl |& tee -a install.log && \\ command -v convert-to-uff 1 2 3 4 5 6 7 Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from uff==0.6.5) (1.18.1) Requirement already satisfied: protobuf>=3.3.0 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from uff==0.6.5) (3.11.4) Requirement already satisfied: six>=1.9 in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.14.0) Requirement already satisfied: setuptools in /home/amins/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (from protobuf>=3.3.0->uff==0.6.5) (46.1.3.post20200325) Installing collected packages: uff Successfully installed uff-0.6.5 Install the Python graphsurgeon wheel file cd ../graphsurgeon && \\ pip install graphsurgeon-0.4.1-py2.py3-none-any.whl |& tee -a install.log 1 2 3 Processing ./graphsurgeon-0.4.1-py2.py3-none-any.whl Installing collected packages: graphsurgeon Successfully installed graphsurgeon-0.4.1 Test compilation and Hello Word example cd ../samples/sampleMNIST && \\ make && \\ cd ../../data/mnist && \\ ## Download MNIST dataset wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz && \\ ls *ubyte.gz | parallel -j2 gunzip {} ./generate_pgms.py && \\ cd ../.. && \\ ./bin/sample_mnist -h && \\ ./bin/sample_mnist --datadir = data/mnist Output shows predicted probability of ASCII formatted input number. After successful install, update Modulefile, s7cuda/toolkit/10.1.243 by adding TensorRT env configs. # TensorRT set tenrthome / projects / verhaak-lab / amins / sumnerenv_os7 / opt / gpu / tensorRT / TensorRT-6.0.1.5 setenv TENSORRT_HOME $tenrthome append - path LD_LIBRARY_PATH $tenrthome / lib append - path PATH $tenrthome / bin Restart terminal and test again conda activate tf-gpu && \\ module load s7cuda/toolkit/10.1.243 && \\ ## sample_mnist will be in PATH sample_mnist --datadir = \" $TENSORRT_HOME \" /data/mnist","title":"TensorRT"},{"location":"conda/S02_gpu_config/#pytorch","text":"Using PyTorch stable build v1.5 for CUDA 10.1, and installing pytorch-1.4.0-py3.7_cuda10.1.243_cudnn7.6.3_0 conda package along with dependencies. Using conda installed CUDA toolkit Unlike tensorflow installation which depends on module load cuda10.1/toolkit/10.1.243 , PyTorch installation will also install CUDA toolkit v10.1. Note that toolkit version must be compatible with GPU card driver, i.e., toolkit v10.1 is compatible with GPU card driver Version: 418.87.01. You can check driver and toolkit compatibility from NVIDIA Drivers Download page. conda activate tf-gpu ## I'd installed matplotlib for other purpose prior to installing pytorch conda activate matplotlib ## Get updated install cmd from https://pytorch.org conda install pytorch torchvision cudatoolkit = 10 .1 -c pytorch Verify installation from __future__ import print_function import torch x = torch . rand ( 5 , 3 ) print ( x ) Above will output a tensor of 5x3 dimensions.","title":"PyTorch"},{"location":"confs/","text":"Configuration files \u00b6 Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Index"},{"location":"confs/#configuration-files","text":"Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Configuration files"},{"location":"confs/dotfiles/","text":"dotfiles \u00b6 Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Index"},{"location":"confs/dotfiles/#dotfiles","text":"Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"dotfiles"},{"location":"confs/dotfiles/hpc_default_env/","text":"Example default dotfiles under user's home directory. File contents may differ.","title":"Default dotfiles"},{"location":"containers/S01_containers/","text":"Singularity","title":"Getting Started"},{"location":"containers/ensembl-vep/","text":"Install VEP via docker \u00b6 Download docker image for vep and convert to singularity .sif format ## v99.2 but may vary for future dates singularity run docker://ensemblorg/ensembl-vep:latest ## OR specific version singularity run docker://ensemblorg/ensembl-vep:release_99.2 Converted image should be under followig path. find \" $SINGULARITY_CACHEDIR \" -type f -name \"ensembl-vep_latest.sif\" I've copied converted sif images to a separate directory and exported directory path as SINGULARITY_SIF env variable. I've renamed ensembl-vep_latest.sif to ensembl-vep_v99.2.sif . Download offline VEP cache \u00b6 To speed up annotations, download vep cache matching vep version downloaded earlier, i.e., v99. See details for downloading cache. cache size can go in several GBs, so save under tier1. cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh38.tar.gz ## approx. 14GB ## MD5: a2a8edfe72ffa659242e66d414027701 homo_sapiens_vep_99_GRCh38.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh38.tar.gz Using hg19/GRCh37 If VCFs for annotations require hg19/GRCh37 assembly, you need to install valid cache for Grch37 assembly. Read discussion here . cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh37.tar.gz ## approx. 13GB ## MD5: 72928de96075666a4477cbd4430084c9 homo_sapiens_vep_99_GRCh37.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh37.tar.gz Run VEP in offline mode \u00b6 Use downloaded cache instead of running in --database mode. The latter will fetch data from online Ensemble database and can be very slow. For detailed arguments, read VEP manpage NOTE: Using --assembly GRCh37 here for hg19 coordinates. Omitting it will default to the most current assembly, GRCh38. singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vep.vcf -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/snv_mnv/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf.gz singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vep.vcf -offline -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/indel/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vcf.gz How to run vcf2maf \u00b6 If you are running vcf2maf, it needs input vcf in uncompressed format. vcf2maf will internally run vep , so you may not need to run vep separately. Read manual before running container Review how-to-run details at mskcc/vcf2maf and by reading manpage. You may endup getting incorrectly parsed maf if certain parameters are not specified per requirement, e.g., --tumor-id and --normal-id requirements for vcfs with tumor, normal samples. singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl --help vcf2maf requires several dependencies. If you get an error, check out this detailed guide . If error persists, please search and/or submit an issue at mskcc/vcf2maf . VEP_DIR = /opt/vep/src/ensembl-vep VEP_DATA = \" ${ RVANNOT } \" /vep_core/v99 REF_FASTA = /projects/verhaak-lab/hg19broad/Homo_sapiens_assembly19.fasta EXAC_VCF = /projects/verhaak-lab/verhaak_env/core_annots/exac/ExAC_nonTCGA.r0.3.1.sites.vep.vcf.gz export VEP_DIR VEP_DATA REF_FASTA EXAC_VCF In contrast to singularity vep image, note the use of singularity exec and not singularity run command for singularity vcf2maf image. Note that this example does NOT use --tumor-id and --normal-id parameters as vcf does not have those columns. However, this may not be a case for vcf output from common somatic callers, like mutect2, varscan2, etc. Increasing vcf-forks more than 4 does not improve conversion exponentially, so there may not be need of increasing threads. singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl \\ --vep-path \" ${ VEP_DIR } \" \\ --vep-data \" ${ VEP_DATA } \" \\ --ref-fasta \" ${ REF_FASTA } \" \\ --filter-vcf \" ${ EXAC_VCF } \" \\ --vep-forks 4 \\ --species homo_sapiens \\ --ncbi-build GRCh37 \\ --input-vcf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf \\ --output-maf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.maf END","title":"How to run VEP using singularity"},{"location":"containers/ensembl-vep/#install-vep-via-docker","text":"Download docker image for vep and convert to singularity .sif format ## v99.2 but may vary for future dates singularity run docker://ensemblorg/ensembl-vep:latest ## OR specific version singularity run docker://ensemblorg/ensembl-vep:release_99.2 Converted image should be under followig path. find \" $SINGULARITY_CACHEDIR \" -type f -name \"ensembl-vep_latest.sif\" I've copied converted sif images to a separate directory and exported directory path as SINGULARITY_SIF env variable. I've renamed ensembl-vep_latest.sif to ensembl-vep_v99.2.sif .","title":"Install VEP via docker"},{"location":"containers/ensembl-vep/#download-offline-vep-cache","text":"To speed up annotations, download vep cache matching vep version downloaded earlier, i.e., v99. See details for downloading cache. cache size can go in several GBs, so save under tier1. cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh38.tar.gz ## approx. 14GB ## MD5: a2a8edfe72ffa659242e66d414027701 homo_sapiens_vep_99_GRCh38.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh38.tar.gz Using hg19/GRCh37 If VCFs for annotations require hg19/GRCh37 assembly, you need to install valid cache for Grch37 assembly. Read discussion here . cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh37.tar.gz ## approx. 13GB ## MD5: 72928de96075666a4477cbd4430084c9 homo_sapiens_vep_99_GRCh37.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh37.tar.gz","title":"Download offline VEP cache"},{"location":"containers/ensembl-vep/#run-vep-in-offline-mode","text":"Use downloaded cache instead of running in --database mode. The latter will fetch data from online Ensemble database and can be very slow. For detailed arguments, read VEP manpage NOTE: Using --assembly GRCh37 here for hg19 coordinates. Omitting it will default to the most current assembly, GRCh38. singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vep.vcf -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/snv_mnv/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf.gz singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vep.vcf -offline -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/indel/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vcf.gz","title":"Run VEP in offline mode"},{"location":"containers/ensembl-vep/#how-to-run-vcf2maf","text":"If you are running vcf2maf, it needs input vcf in uncompressed format. vcf2maf will internally run vep , so you may not need to run vep separately. Read manual before running container Review how-to-run details at mskcc/vcf2maf and by reading manpage. You may endup getting incorrectly parsed maf if certain parameters are not specified per requirement, e.g., --tumor-id and --normal-id requirements for vcfs with tumor, normal samples. singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl --help vcf2maf requires several dependencies. If you get an error, check out this detailed guide . If error persists, please search and/or submit an issue at mskcc/vcf2maf . VEP_DIR = /opt/vep/src/ensembl-vep VEP_DATA = \" ${ RVANNOT } \" /vep_core/v99 REF_FASTA = /projects/verhaak-lab/hg19broad/Homo_sapiens_assembly19.fasta EXAC_VCF = /projects/verhaak-lab/verhaak_env/core_annots/exac/ExAC_nonTCGA.r0.3.1.sites.vep.vcf.gz export VEP_DIR VEP_DATA REF_FASTA EXAC_VCF In contrast to singularity vep image, note the use of singularity exec and not singularity run command for singularity vcf2maf image. Note that this example does NOT use --tumor-id and --normal-id parameters as vcf does not have those columns. However, this may not be a case for vcf output from common somatic callers, like mutect2, varscan2, etc. Increasing vcf-forks more than 4 does not improve conversion exponentially, so there may not be need of increasing threads. singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl \\ --vep-path \" ${ VEP_DIR } \" \\ --vep-data \" ${ VEP_DATA } \" \\ --ref-fasta \" ${ REF_FASTA } \" \\ --filter-vcf \" ${ EXAC_VCF } \" \\ --vep-forks 4 \\ --species homo_sapiens \\ --ncbi-build GRCh37 \\ --input-vcf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf \\ --output-maf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.maf END","title":"How to run vcf2maf"},{"location":"slurm/S01_slurm101/","text":"Slurm cheatsheet Slurm cheatsheet Tutorials Available options for sbatch Available options for srun Running jobs at Sherlock at Stanford Running jobs at Yale Research Computing mksbatch \u00b6 Bash wrapper to make slurm sbatch jobscript. Download mksbatch script Require dos2unix command mksbatch requires dos2unix command to make sure converted sbatch file is in unix-compliant text format. If you do not have it, you may try commenting it out before running the command. Put in ~/bin/ with chmod 755 ~/bin/mksbatch Make an example text file with bash commands you like to run, e.g., nano ~/mycmds.txt with following contents: echo \"This will be run as sbatch job\" echo \"List contents under /projects/\" ls /projects/ echo \"print out user env\" env sleep 10 echo \"Good bye!\" To convert mycmds.txt into slurm sbatch compatible format mksbatch -a ~/mycmds.txt This will make ~/mycmds .sbatch* script with default job resources. To alter job resources, checkout help section for available options. mksbatch --help Wrapper to make slurm sbatch job format on HPC Sumner at JAX. For options, read sbatch manpage at https://slurm.schedmd.com/sbatch.html Usage: mksbatch -a <path to files containing commands> -h display this help and exit -a REQUIRED: path to file containing commands to be run on cluster. This file will be copied verbatim following SBATCH arguments. -j job name (default: j<random id>_username) -w work directory (default: present work directory) -P job partition (default: batch; compute,gpu) -q job queue (default: batch) -t walltime in HH:MM:SS (default: 01:00:00) -m memory in gb (default: 4G) -n number of nodes (default: 1) -c cpu cores per node (default: 1) -p email notifications (default: FAIL; NONE, BEGIN, END, FAIL, REQUEUE, ALL) -s if set to YES, export TMPDIR to /fastscratch/user/tmp (default: NO) -e extra options to SBATCH (will be appended to default ones: \"--requeue --export=all\") -x Submit job (default: NO; YES to submit) Example: mksbatch -j \"job1\" -t 01:00:00 -m 12gb -c 1 -a ~/mycmds.txt Quotes are important for variable names containig spaces and special characters.","title":"Slurm 101"},{"location":"slurm/S01_slurm101/#mksbatch","text":"Bash wrapper to make slurm sbatch jobscript. Download mksbatch script Require dos2unix command mksbatch requires dos2unix command to make sure converted sbatch file is in unix-compliant text format. If you do not have it, you may try commenting it out before running the command. Put in ~/bin/ with chmod 755 ~/bin/mksbatch Make an example text file with bash commands you like to run, e.g., nano ~/mycmds.txt with following contents: echo \"This will be run as sbatch job\" echo \"List contents under /projects/\" ls /projects/ echo \"print out user env\" env sleep 10 echo \"Good bye!\" To convert mycmds.txt into slurm sbatch compatible format mksbatch -a ~/mycmds.txt This will make ~/mycmds .sbatch* script with default job resources. To alter job resources, checkout help section for available options. mksbatch --help Wrapper to make slurm sbatch job format on HPC Sumner at JAX. For options, read sbatch manpage at https://slurm.schedmd.com/sbatch.html Usage: mksbatch -a <path to files containing commands> -h display this help and exit -a REQUIRED: path to file containing commands to be run on cluster. This file will be copied verbatim following SBATCH arguments. -j job name (default: j<random id>_username) -w work directory (default: present work directory) -P job partition (default: batch; compute,gpu) -q job queue (default: batch) -t walltime in HH:MM:SS (default: 01:00:00) -m memory in gb (default: 4G) -n number of nodes (default: 1) -c cpu cores per node (default: 1) -p email notifications (default: FAIL; NONE, BEGIN, END, FAIL, REQUEUE, ALL) -s if set to YES, export TMPDIR to /fastscratch/user/tmp (default: NO) -e extra options to SBATCH (will be appended to default ones: \"--requeue --export=all\") -x Submit job (default: NO; YES to submit) Example: mksbatch -j \"job1\" -t 01:00:00 -m 12gb -c 1 -a ~/mycmds.txt Quotes are important for variable names containig spaces and special characters.","title":"mksbatch"},{"location":"slurm/how_to_interactive_via_tmux/","text":"Each time you login to Sumner HPC, you will be redirected to one of two login nodes. If you are using tmux and running interactive job (using slurm) on a compute node, you may prefer to detach it, say during lunch or meeting times and like to re-attach afterwards from your local machine. If so, you do need to remember on which of two login nodes your tmux session was running, and login to that node and re-attach tmux session. This bash wrapper around tmux and slurm scheduler will (for the most part) do that job. Using tmux -V . I've tested it with v2.9+ Copy start_interactive from docs/confs/bin directory to ~/bin/start_interactive and chmod 755 start_interactive . If needed, change default resources and walltime for interactive job command where variable INTJOB is specified. This script will change move TMUX_TMPDIR from /tmp to ~/logs/tmux and bind tmux session with session ID specific to login-node, e.g., S1 and S2 for respectively, login-node1 and login-node2. From your local machine, place a bash wrapper like ~/bin/run_interactive_hpc with following code, and do chmod 755 ~/bin/run_interactive_hpc You may need to add/edit sumner profile in local ~/.ssh/config . Local ~/.ssh/config file ## example config for sumner. Host sumner Hostname login.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no Host sumner1 Hostname sumner-log1.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no Host sumner2 Hostname sumner-log2.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no #!/bin/bash ## Run or attach to interactive session on one of sumner login nodes ssh sumner -t 'bash -l -c \"start_interactive\"' exitstat1 = $? if [[ \" $exitstat1 \" ! = 0 ]] ; then echo -e \"\\n#####\\nAttempt one more time to match login node and tmux session\\n#####\\n\" sleep 5 ## extract stderr for parsing remote hostname ssh sumner -t 'bash -l -c \"start_interactive\"' > | /tmp/sumner_interactive exitstat2 = $? if [[ \" $exitstat2 \" ! = 0 ]] ; then echo -e \"\\nForce ssh to login node matching tmux session.\" ## get login node where tmux is running TMUXHOST = \" $( grep -Eo \"S[12]{1}INT\" /tmp/sumner_interactive ) \" echo -e \"\\ntmux session ID is ${ TMUXHOST } \\nForce login to server where tmux is running.\" ## Using ~/.ssh/config profile for a specific login node if [[ \" ${ TMUXHOST } \" == \"S1INT\" ]] ; then ssh sumner1 -t 'bash -l -c \"start_interactive\"' elif [[ \" ${ TMUXHOST } \" == \"S2INT\" ]] ; then ssh sumner2 -t 'bash -l -c \"start_interactive\"' else echo -e \"\\nERROR: Invalid pattern for remote tmux session ID: ${ TMUXHOST } \\nIt should be either S1INT or S2INT.\\nCan not start_interactive session.\" > & 2 exit 1 fi fi fi ## END ## From your local machine, run ~/bin/run_interactive_hpc and see if it works! If not, feel free to raise question at user forum . Notes Avoid running start_interactive while you are on login node or spawning new interactive job inside a custom tmux session. Instead, prefer running ~/bin/run_interactive_hpc from your local machine . Wrapper allows running only one interactive job with login-node specific job name, and relies on login-node specific tmux session else it will fail to work. If you like to go back to detached tmux session from login node, you can instead use tmux new-session -A -s S1 (or S2 for login-node2) to attach to either running session or create a new one. Details here or checkout docs/confs/bin/tmx .","title":"Interactive job inside tmux"},{"location":"slurm/how_to_snakemake_slurm/","text":"Toy example \u00b6 Compliant with JAX HPC Sumner (Cent OS7) using slurm v18.08.8 and Snakemake v5.14.0 Setup \u00b6 ssh sumner and clone TheJacksonLaboratory/toymake repository. mkdir -p ~/pipelines/snakemake cd ~/pipelines/snakemake git clone git@github.com:TheJacksonLaboratory/toymake.git cd toymake Edit config.yaml to match your username and valid path for smk_home (where snakemake code resides, typically tier1 space) and workdir (where snakemake output will be stored, typically scratch space). Edit Snakefile to match your username. Setup slurm profile for snakemake. Read details here first mkdir -p ~/.config/snakemake/sumner/ rsync -avhP ~/pipelines/snakemake/toymake/profile/sumner/ ~/.config/snakemake/sumner/ To modify job resources, edit config/slurm_sumner_defaults.yaml Order of Precedence for sbatch arguments \u00b6 Arguments to sbatch takes precedence in the following order (lowest to highest priority) and must be named according to sbatch long option names . Please read details here first . sbatch_defaults in ~/.config/snakemake/sumner/slurm-submit.py Profile cluster_config file __default__ entries from config/slurm_sumner_defaults.yaml Snakefile threads and resources (time, mem_mb) Profile cluster_config file <rulename> entries from config/slurm_sumner_defaults.yaml --cluster-config parsed to Snakemake: Avoid this as it is deprecated since Snakemake 5.10. Run workflow \u00b6 run_snakemake.sh is a wrapper around snakemake command. It exports environment variables, SMKDIR and WORKDIR , which are internally used by sumner slurm profile at ~/.config/snakemake/sumner/ ./run_snakemake.sh Expected output Credits \u00b6 Snakemake-Profiles/slurm by Per Unneberg Variant of Snakemake slurm profile by Ben Parks","title":"How To Run Snakemake"},{"location":"slurm/how_to_snakemake_slurm/#toy-example","text":"Compliant with JAX HPC Sumner (Cent OS7) using slurm v18.08.8 and Snakemake v5.14.0","title":"Toy example"},{"location":"slurm/how_to_snakemake_slurm/#setup","text":"ssh sumner and clone TheJacksonLaboratory/toymake repository. mkdir -p ~/pipelines/snakemake cd ~/pipelines/snakemake git clone git@github.com:TheJacksonLaboratory/toymake.git cd toymake Edit config.yaml to match your username and valid path for smk_home (where snakemake code resides, typically tier1 space) and workdir (where snakemake output will be stored, typically scratch space). Edit Snakefile to match your username. Setup slurm profile for snakemake. Read details here first mkdir -p ~/.config/snakemake/sumner/ rsync -avhP ~/pipelines/snakemake/toymake/profile/sumner/ ~/.config/snakemake/sumner/ To modify job resources, edit config/slurm_sumner_defaults.yaml","title":"Setup"},{"location":"slurm/how_to_snakemake_slurm/#order-of-precedence-for-sbatch-arguments","text":"Arguments to sbatch takes precedence in the following order (lowest to highest priority) and must be named according to sbatch long option names . Please read details here first . sbatch_defaults in ~/.config/snakemake/sumner/slurm-submit.py Profile cluster_config file __default__ entries from config/slurm_sumner_defaults.yaml Snakefile threads and resources (time, mem_mb) Profile cluster_config file <rulename> entries from config/slurm_sumner_defaults.yaml --cluster-config parsed to Snakemake: Avoid this as it is deprecated since Snakemake 5.10.","title":"Order of Precedence for sbatch arguments"},{"location":"slurm/how_to_snakemake_slurm/#run-workflow","text":"run_snakemake.sh is a wrapper around snakemake command. It exports environment variables, SMKDIR and WORKDIR , which are internally used by sumner slurm profile at ~/.config/snakemake/sumner/ ./run_snakemake.sh Expected output","title":"Run workflow"},{"location":"slurm/how_to_snakemake_slurm/#credits","text":"Snakemake-Profiles/slurm by Per Unneberg Variant of Snakemake slurm profile by Ben Parks","title":"Credits"}]}