{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Template for HPC Sumner Documentation Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment.","title":"Home"},{"location":"conda/S01_conda/","text":"Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment. First-time Login \u00b6 1 2 3 4 5 ssh login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log2 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to /confs/dotfiles/hpc_default_env/ . 1 2 3 4 ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), comment those out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and linuxbrew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to this one. Note that PATH variable will default to cent os 7 standard paths and LD_LIBRARY_PATH should preferably only contain entries related to slurm scheduler. 1 2 3 4 exit #from sumner ## login again ssh login.sumner.jax.org Check common env variables set by default (HPC team) 1 2 ## paths where all executables can be found echo $PATH 1 /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/yoda/.local/bin:/home/yoda/bin 1 2 ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH 1 /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 1 2 3 ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH 1 /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 1 2 3 ## default loaded modules ## Most of paths in PATH, LD_LIBRARY_PATH, and LIBRARY_PATH are configured by these loaded modules. module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) dot 4) default-environment 5) slurm/18.08.8 6) gcc/8.2.0 Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! 1 2 3 4 5 6 7 mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a ~/bkup/confs/hpc_default_env/default_hpc_env.txt dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). 1 module list For now, you may add following cmd to your ~/.bash_profile . Eventually it will go to ~/.profile.d/ setup detailed below. 1 module unload dot For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload gcc first. 1 2 module unload gcc module list 1 2 Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 Unloading gcc Note that while starting pseudo-terminal using screen, tmux, or slurm interactive job, you may get module loaded gcc again in PATH. If so, make sure to do module unload gcc before running setup further. Backup and Reset Dotfiles \u00b6 Move dotfiles to archived directory If you have dotfiles and/or dot directories like .Renviron, .Rprofile, .curlrc, .cache, .config, etc., that may cause issues configuring environment. Moving dotfiles Following is what I've done but may not be safe unless you know what you are doing! Moving some of dotfiles is tricky as they are required for login to sumner. If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## WARN: Moving all files and directories starting ## with dot to archived dir. mkdir -p \" ${ HOME } \" /legacy_env && \\ mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ ## DO NOT FORGET TO COPY BACK following files to ## \"${HOME}\"/ else you may get locked out of sumner. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.bash* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ rsync -avhP legacy_env/.parallel ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ ## make empty dirs ## note that user .local and .config, if any are now backed up and ## we are creating a empty ~/.local directory mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## CONFIRM FROM A SEPARATE TERMINAL that you can login to sumner ssh yoda@sumner env If above command succeeds and env looks similar (PATH in particular) to PATH and LD_LIBRARY_PATH shown above, you're good! You can exit old sumner session and install anaconda3 in new terminal session. Start Interactive Job prior to installation Prefer running setup on a dedicated interactive node instead of login node. Some of conda install/update steps may get killed on login node. 1 srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Install anaconda3 \u00b6 Python3 over Python2 Prefer using Anaconda3 (Python3) over Python2 as the latter has reached End of Life . For anaconda3: Using v2019.03-Linux-x86_64 with permalink and MD5 : b77a71c3712b45c8f33c7b2ecade366c. 1 2 3 4 5 cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ md5sum Anaconda3-2019.10-Linux-x86_64.sh Setup anaconda3 environment with default options, i.e., install at ~/anaconda3 . 1 2 cd \" $HOME \" && \\ bash ~/Downloads/conda/Anaconda3-2019.10-Linux-x86_64.sh Once conda installation is complete at default location, logout and login to sumner. Default conda env, base will now be in effect. Note (base) prefix to your username. Also, check output of following: 1 echo $CONDA_DEFAULT_ENV base 1 echo $CONDA_PREFIX /home/yoda/anaconda3 Configuring Conda \u00b6 Following are additional configuration within conda environment to enable installation of R 3.6.1+ and Jupyter. Set channel priority \u00b6 Important Prefer installing compilers only from a single channel and avoid mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled at default for conda v4.6 or higher. Add Bioconda and conda-forge channels to get updated and compbio related packages. Avoid changing order of following commands unless you prefer to keep a different priority for these channels. 1 2 3 conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and set priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as shown below. This file should be present after above commands and no need to edit unless changing priority of channels. This is in yaml format file. Please take care of preceding spaces (and not tabs) before and after - while editing this file. 1 2 3 4 channels: - conda-forge - bioconda - defaults Install R 3.6.1 \u00b6 For compatibility with conda env, prefer installing R 3.6 via conda. Prefer installing R env from conda-forge channel which is already a priority channel in our ~/.condarc . We also specify R version, just to make sure that we install R v3.6.1. Beware of non-standard conda packages Note that most up-to-date R version may be available in the same or other conda channels, like r or bioconda but it is preferable to install R from the first priority channel, i.e., conda-forge in our case, and where package does not show non-standard labels at anaconda website, e.g., As of writing this section, R 3.6.2 on anaconda website shows use of non-standard labels, most likely because it was updated a few days before and so, may not comply with all of conda dependency. 1 2 3 4 ## look for a line r-base and check source channel. ## If it is other than conda-forge, try to downgrade R package where ## r-base is available under conda-forge channel. conda install -c conda-forge r-base = 3 .6.1 Above command will take 15-30 min to resolve dependencies. It will do major overhaul of default conda environment by... upgrading conda to latest version, 4.8.2 or higher. installing conda and r-base from conda-forge source. and much more... Pin R and conda auto-updates \u00b6 Before moving further, let's pin R version to 3.6.1 and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do conda install <pkg> in future, and carefully install/update packages without breaking existing setup. Compile over conda install Rpkg Typically, I avoid installing or updating package if conda install throws a message or warning about removing or downgrading existing packages. In such cases, I fall back to compiling package using available devtools in conda and sumner. Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. 1 2 3 4 conda config --set auto_update_conda False ## add following to pinned file nano ~/anaconda3/conda-meta/pinned r-base ==3.6.1 Setup Rprofile and Renviron \u00b6 Setup R package directory path for R 3.6 1 mkdir -p ~/R/pkgs/v3.6 Create these two files to setup startup env for R Renviron 1 nano ~/.Renviron Where should R save new packages? Over time, this directory may grow in size, so better to keep under tier1 storage. 1 R_LIBS = \"/projects/verhaak-lab/yoda/sumnerenv_os7/R/pkgs/v3.6:/home/yoda/R/pkgs/v3.6:/home/yoda/anaconda3/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() command. Note that /home/yoda/anaconda3/lib/R/library is a required path set while installing R using conda. R profile 1 nano ~/.Rprofile 1 2 3 4 5 6 7 8 9 10 ## set user specific env variables, if any here ## e.g., GITHUB_PAT if here Sys.setenv(\"GITHUB_PAT\"=\"xyzabc1234\") ## Default source to download packages local({ r <- getOption(\"repos\") r[\"CRAN\"] <- \"https://cran.rstudio.com\" options(repos = r) }) Before further configuring sumner env, let's logout and login first from interactive job and exit sumner. Then, login back to sumner and start interactive queue again. 1 2 3 4 5 6 7 exit # from interactive session exit # from sumner ssh sumner ## start interactive session srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Confirm that login env is similar to earlier env (just after anaconda3 setup) by running env command. PATH should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc. Also, make sure that module: gcc is unloaded. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools. 1 2 3 4 5 6 ## unload gcc if loaded module unload gcc ## example PATH variable /home/yoda/anaconda3/bin:/home/yoda/anaconda3/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin :/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/yoda/.local/bin:/home/yoda/bin Avoid Rpkgs using conda install Install essential R packages and jupyter kernel for R only if they are available for R 3.6.x (preferably v3.6.1) from conda-forge channel. This may not be a case for most times, e.g., for R v3.5.1, I can find compatible packages but not for R 3.6. I avoid downloading r-essentials from other channels, including r as some packages have dependencies on shared library which are different between r and conda-forge channel. I prefer installing R packages using install.packages , devtools::install_github() or BiocManager command in R. 1 2 ## This did not work for R 3.6.1 # conda install -c conda-forge r-essentials Installing R and linux packages \u00b6 Here, I alternate between R and bash to install packages I use on regular basis. It is going to take a while to install these packages. You may not need to install all packages but if you are skipping ahead and get into error because of missing library, package, etc., you probably need to revisit this code block and confirm that you have all of dependencies in your environment for successful installation. 1 install.packages ( \"devtools\" ) When you start installing R packages, notice x86_64-conda_cos6-linux-gnu-cc or other compilers that R is using instead of /usr/bin/gcc or sumner-defaults. Similarly, during loading R packages which requires shared libraries, we link shared libs from conda and not from system-defaults (gcc and others). That's one of reasons I did module unload gcc before installing R packages. Eventually, we will set ~/.profile.d environment such that it will always load conda environment and ignore gcc and other devtools from system paths. Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment. Backup conda env \u00b6 Base environment. 1 2 ## script is on github repo under conds/bin/ ./conda_bkup.sh","title":"Set Up Conda"},{"location":"conda/S01_conda/#first-time-login","text":"1 2 3 4 5 ssh login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log2 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to /confs/dotfiles/hpc_default_env/ . 1 2 3 4 ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), comment those out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and linuxbrew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to this one. Note that PATH variable will default to cent os 7 standard paths and LD_LIBRARY_PATH should preferably only contain entries related to slurm scheduler. 1 2 3 4 exit #from sumner ## login again ssh login.sumner.jax.org Check common env variables set by default (HPC team) 1 2 ## paths where all executables can be found echo $PATH 1 /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/yoda/.local/bin:/home/yoda/bin 1 2 ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH 1 /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 1 2 3 ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH 1 /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 1 2 3 ## default loaded modules ## Most of paths in PATH, LD_LIBRARY_PATH, and LIBRARY_PATH are configured by these loaded modules. module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) dot 4) default-environment 5) slurm/18.08.8 6) gcc/8.2.0 Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! 1 2 3 4 5 6 7 mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a ~/bkup/confs/hpc_default_env/default_hpc_env.txt dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). 1 module list For now, you may add following cmd to your ~/.bash_profile . Eventually it will go to ~/.profile.d/ setup detailed below. 1 module unload dot For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload gcc first. 1 2 module unload gcc module list 1 2 Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 Unloading gcc Note that while starting pseudo-terminal using screen, tmux, or slurm interactive job, you may get module loaded gcc again in PATH. If so, make sure to do module unload gcc before running setup further.","title":"First-time Login"},{"location":"conda/S01_conda/#backup-and-reset-dotfiles","text":"Move dotfiles to archived directory If you have dotfiles and/or dot directories like .Renviron, .Rprofile, .curlrc, .cache, .config, etc., that may cause issues configuring environment. Moving dotfiles Following is what I've done but may not be safe unless you know what you are doing! Moving some of dotfiles is tricky as they are required for login to sumner. If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## WARN: Moving all files and directories starting ## with dot to archived dir. mkdir -p \" ${ HOME } \" /legacy_env && \\ mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ ## DO NOT FORGET TO COPY BACK following files to ## \"${HOME}\"/ else you may get locked out of sumner. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.bash* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ rsync -avhP legacy_env/.parallel ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ ## make empty dirs ## note that user .local and .config, if any are now backed up and ## we are creating a empty ~/.local directory mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## CONFIRM FROM A SEPARATE TERMINAL that you can login to sumner ssh yoda@sumner env If above command succeeds and env looks similar (PATH in particular) to PATH and LD_LIBRARY_PATH shown above, you're good! You can exit old sumner session and install anaconda3 in new terminal session. Start Interactive Job prior to installation Prefer running setup on a dedicated interactive node instead of login node. Some of conda install/update steps may get killed on login node. 1 srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash","title":"Backup and Reset Dotfiles"},{"location":"conda/S01_conda/#install-anaconda3","text":"Python3 over Python2 Prefer using Anaconda3 (Python3) over Python2 as the latter has reached End of Life . For anaconda3: Using v2019.03-Linux-x86_64 with permalink and MD5 : b77a71c3712b45c8f33c7b2ecade366c. 1 2 3 4 5 cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ md5sum Anaconda3-2019.10-Linux-x86_64.sh Setup anaconda3 environment with default options, i.e., install at ~/anaconda3 . 1 2 cd \" $HOME \" && \\ bash ~/Downloads/conda/Anaconda3-2019.10-Linux-x86_64.sh Once conda installation is complete at default location, logout and login to sumner. Default conda env, base will now be in effect. Note (base) prefix to your username. Also, check output of following: 1 echo $CONDA_DEFAULT_ENV base 1 echo $CONDA_PREFIX /home/yoda/anaconda3","title":"Install anaconda3"},{"location":"conda/S01_conda/#configuring-conda","text":"Following are additional configuration within conda environment to enable installation of R 3.6.1+ and Jupyter.","title":"Configuring Conda"},{"location":"conda/S01_conda/#set-channel-priority","text":"Important Prefer installing compilers only from a single channel and avoid mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled at default for conda v4.6 or higher. Add Bioconda and conda-forge channels to get updated and compbio related packages. Avoid changing order of following commands unless you prefer to keep a different priority for these channels. 1 2 3 conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and set priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as shown below. This file should be present after above commands and no need to edit unless changing priority of channels. This is in yaml format file. Please take care of preceding spaces (and not tabs) before and after - while editing this file. 1 2 3 4 channels: - conda-forge - bioconda - defaults","title":"Set channel priority"},{"location":"conda/S01_conda/#install-r-361","text":"For compatibility with conda env, prefer installing R 3.6 via conda. Prefer installing R env from conda-forge channel which is already a priority channel in our ~/.condarc . We also specify R version, just to make sure that we install R v3.6.1. Beware of non-standard conda packages Note that most up-to-date R version may be available in the same or other conda channels, like r or bioconda but it is preferable to install R from the first priority channel, i.e., conda-forge in our case, and where package does not show non-standard labels at anaconda website, e.g., As of writing this section, R 3.6.2 on anaconda website shows use of non-standard labels, most likely because it was updated a few days before and so, may not comply with all of conda dependency. 1 2 3 4 ## look for a line r-base and check source channel. ## If it is other than conda-forge, try to downgrade R package where ## r-base is available under conda-forge channel. conda install -c conda-forge r-base = 3 .6.1 Above command will take 15-30 min to resolve dependencies. It will do major overhaul of default conda environment by... upgrading conda to latest version, 4.8.2 or higher. installing conda and r-base from conda-forge source. and much more...","title":"Install R 3.6.1"},{"location":"conda/S01_conda/#pin-r-and-conda-auto-updates","text":"Before moving further, let's pin R version to 3.6.1 and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do conda install <pkg> in future, and carefully install/update packages without breaking existing setup. Compile over conda install Rpkg Typically, I avoid installing or updating package if conda install throws a message or warning about removing or downgrading existing packages. In such cases, I fall back to compiling package using available devtools in conda and sumner. Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. 1 2 3 4 conda config --set auto_update_conda False ## add following to pinned file nano ~/anaconda3/conda-meta/pinned r-base ==3.6.1","title":"Pin R and conda auto-updates"},{"location":"conda/S01_conda/#setup-rprofile-and-renviron","text":"Setup R package directory path for R 3.6 1 mkdir -p ~/R/pkgs/v3.6 Create these two files to setup startup env for R Renviron 1 nano ~/.Renviron Where should R save new packages? Over time, this directory may grow in size, so better to keep under tier1 storage. 1 R_LIBS = \"/projects/verhaak-lab/yoda/sumnerenv_os7/R/pkgs/v3.6:/home/yoda/R/pkgs/v3.6:/home/yoda/anaconda3/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() command. Note that /home/yoda/anaconda3/lib/R/library is a required path set while installing R using conda. R profile 1 nano ~/.Rprofile 1 2 3 4 5 6 7 8 9 10 ## set user specific env variables, if any here ## e.g., GITHUB_PAT if here Sys.setenv(\"GITHUB_PAT\"=\"xyzabc1234\") ## Default source to download packages local({ r <- getOption(\"repos\") r[\"CRAN\"] <- \"https://cran.rstudio.com\" options(repos = r) }) Before further configuring sumner env, let's logout and login first from interactive job and exit sumner. Then, login back to sumner and start interactive queue again. 1 2 3 4 5 6 7 exit # from interactive session exit # from sumner ssh sumner ## start interactive session srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash Confirm that login env is similar to earlier env (just after anaconda3 setup) by running env command. PATH should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc. Also, make sure that module: gcc is unloaded. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools. 1 2 3 4 5 6 ## unload gcc if loaded module unload gcc ## example PATH variable /home/yoda/anaconda3/bin:/home/yoda/anaconda3/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin :/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/yoda/.local/bin:/home/yoda/bin Avoid Rpkgs using conda install Install essential R packages and jupyter kernel for R only if they are available for R 3.6.x (preferably v3.6.1) from conda-forge channel. This may not be a case for most times, e.g., for R v3.5.1, I can find compatible packages but not for R 3.6. I avoid downloading r-essentials from other channels, including r as some packages have dependencies on shared library which are different between r and conda-forge channel. I prefer installing R packages using install.packages , devtools::install_github() or BiocManager command in R. 1 2 ## This did not work for R 3.6.1 # conda install -c conda-forge r-essentials","title":"Setup Rprofile and Renviron"},{"location":"conda/S01_conda/#installing-r-and-linux-packages","text":"Here, I alternate between R and bash to install packages I use on regular basis. It is going to take a while to install these packages. You may not need to install all packages but if you are skipping ahead and get into error because of missing library, package, etc., you probably need to revisit this code block and confirm that you have all of dependencies in your environment for successful installation. 1 install.packages ( \"devtools\" ) When you start installing R packages, notice x86_64-conda_cos6-linux-gnu-cc or other compilers that R is using instead of /usr/bin/gcc or sumner-defaults. Similarly, during loading R packages which requires shared libraries, we link shared libs from conda and not from system-defaults (gcc and others). That's one of reasons I did module unload gcc before installing R packages. Eventually, we will set ~/.profile.d environment such that it will always load conda environment and ignore gcc and other devtools from system paths. Work In Progress Documentation is in alpha stage and not intended for setting up Sumner HPC environment.","title":"Installing R and linux packages"},{"location":"conda/S01_conda/#backup-conda-env","text":"Base environment. 1 2 ## script is on github repo under conds/bin/ ./conda_bkup.sh","title":"Backup conda env"},{"location":"confs/","text":"Configuration files \u00b6 Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Home"},{"location":"confs/#configuration-files","text":"Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Configuration files"},{"location":"confs/dotfiles/","text":"dotfiles \u00b6 Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"Home"},{"location":"confs/dotfiles/#dotfiles","text":"Configuration files used during setup. Please do not copy and paste without reading documentation else this may break your login environment.","title":"dotfiles"},{"location":"confs/dotfiles/hpc_default_env/","text":"Example default dotfiles under user's home directory. File contents may differ.","title":"Default dotfiles"},{"location":"containers/S01_containers/","text":"Singularity","title":"Getting Started"},{"location":"containers/ensembl-vep/","text":"For HPC Sumner at JAX @sbamin Install VEP via docker \u00b6 Download docker image for vep and convert to singularity .sif format 1 2 3 4 ## v99.2 but may vary for future dates singularity run docker://ensemblorg/ensembl-vep:latest ## OR specific version singularity run docker://ensemblorg/ensembl-vep:release_99.2 Converted image should be under followig path. 1 find \" $SINGULARITY_CACHEDIR \" -type f -name \"ensembl-vep_latest.sif\" I've copied converted sif images to a separate directory and exported directory path as SINGULARITY_SIF env variable. I've renamed ensembl-vep_latest.sif to ensembl-vep_v99.2.sif . Download offline VEP cache \u00b6 To speed up annotations, download vep cache matching vep version downloaded earlier, i.e., v99. See details for downloading cache. cache size can go in several GBs, so save under tier1. 1 2 3 4 5 6 7 8 cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh38.tar.gz ## approx. 14GB ## MD5: a2a8edfe72ffa659242e66d414027701 homo_sapiens_vep_99_GRCh38.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh38.tar.gz IMPORTANT: Using hg19/GRCh37 \u00b6 If VCFs for annotations require hg19/GRCh37 assembly, you need to install valid cache for Grch37 assembly. Read discussion here . 1 2 3 4 5 6 7 8 cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh37.tar.gz ## approx. 13GB ## MD5: 72928de96075666a4477cbd4430084c9 homo_sapiens_vep_99_GRCh37.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh37.tar.gz Run VEP in offline mode \u00b6 Use downloaded cache instead of running in --database mode. The latter will fetch data from online Ensemble database and can be very slow. For detailed arguments, read VEP manpage NOTE: Using --assembly GRCh37 here for hg19 coordinates. Omitting it will default to the most current assembly, GRCh38. 1 2 3 singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vep.vcf -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/snv_mnv/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf.gz singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vep.vcf -offline -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/indel/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vcf.gz How to run vcf2maf \u00b6 If you are running vcf2maf, it needs input vcf in uncompressed format. vcf2maf will internally run vep , so you may not need to run vep separately. Review how-to-run details at mskcc/vcf2maf and by reading manpage. You may endup getting incorrectly parsed maf if certain parameters are not specified per requirement, e.g., --tumor-id and --normal-id requirements for vcfs with tumor, normal samples. 1 singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl --help vcf2maf requires several dependencies. If you get an error, check out this detailed guide . If error persists, please search and/or submit an issue at mskcc/vcf2maf . 1 2 3 4 5 6 VEP_DIR = /opt/vep/src/ensembl-vep VEP_DATA = \" ${ RVANNOT } \" /vep_core/v99 REF_FASTA = /projects/verhaak-lab/hg19broad/Homo_sapiens_assembly19.fasta EXAC_VCF = /projects/verhaak-lab/verhaak_env/core_annots/exac/ExAC_nonTCGA.r0.3.1.sites.vep.vcf.gz export VEP_DIR VEP_DATA REF_FASTA EXAC_VCF In contrast to singularity vep image, note the use of singularity exec and not singularity run command for singularity vcf2maf image. Note that this example does NOT use --tumor-id and --normal-id parameters as vcf does not have those columns. However, this may not be a case for vcf output from common somatic callers, like mutect2, varscan2, etc. Increasing vcf-forks more than 4 does not improve conversion exponentially, so there may not be need of increasing threads. 1 2 3 4 5 6 7 8 9 10 singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl \\ --vep-path \" ${ VEP_DIR } \" \\ --vep-data \" ${ VEP_DATA } \" \\ --ref-fasta \" ${ REF_FASTA } \" \\ --filter-vcf \" ${ EXAC_VCF } \" \\ --vep-forks 4 \\ --species homo_sapiens \\ --ncbi-build GRCh37 \\ --input-vcf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf \\ --output-maf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.maf END","title":"How to run VEP using singularity"},{"location":"containers/ensembl-vep/#install-vep-via-docker","text":"Download docker image for vep and convert to singularity .sif format 1 2 3 4 ## v99.2 but may vary for future dates singularity run docker://ensemblorg/ensembl-vep:latest ## OR specific version singularity run docker://ensemblorg/ensembl-vep:release_99.2 Converted image should be under followig path. 1 find \" $SINGULARITY_CACHEDIR \" -type f -name \"ensembl-vep_latest.sif\" I've copied converted sif images to a separate directory and exported directory path as SINGULARITY_SIF env variable. I've renamed ensembl-vep_latest.sif to ensembl-vep_v99.2.sif .","title":"Install VEP via docker"},{"location":"containers/ensembl-vep/#download-offline-vep-cache","text":"To speed up annotations, download vep cache matching vep version downloaded earlier, i.e., v99. See details for downloading cache. cache size can go in several GBs, so save under tier1. 1 2 3 4 5 6 7 8 cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh38.tar.gz ## approx. 14GB ## MD5: a2a8edfe72ffa659242e66d414027701 homo_sapiens_vep_99_GRCh38.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh38.tar.gz","title":"Download offline VEP cache"},{"location":"containers/ensembl-vep/#important-using-hg19grch37","text":"If VCFs for annotations require hg19/GRCh37 assembly, you need to install valid cache for Grch37 assembly. Read discussion here . 1 2 3 4 5 6 7 8 cd \" $RVANNOT \" /vep_core/v99 curl -O ftp://ftp.ensembl.org/pub/release-99/variation/indexed_vep_cache/homo_sapiens_vep_99_GRCh37.tar.gz ## approx. 13GB ## MD5: 72928de96075666a4477cbd4430084c9 homo_sapiens_vep_99_GRCh37.tar.gz ## extract cache tar xzf homo_sapiens_vep_99_GRCh37.tar.gz","title":"IMPORTANT: Using hg19/GRCh37"},{"location":"containers/ensembl-vep/#run-vep-in-offline-mode","text":"Use downloaded cache instead of running in --database mode. The latter will fetch data from online Ensemble database and can be very slow. For detailed arguments, read VEP manpage NOTE: Using --assembly GRCh37 here for hg19 coordinates. Omitting it will default to the most current assembly, GRCh38. 1 2 3 singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vep.vcf -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/snv_mnv/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf.gz singularity run \" ${ SINGULARITY_SIF } \" /ensembl-vep_v99.2.sif vep --offline --vcf --dir_cache \" ${ RVANNOT } /vep_core/v99\" --species homo_sapiens --assembly GRCh37 -o ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vep.vcf -offline -i /projects/verhaak-lab/ecdna/datasets/pcawg/dump/final_consensus_12oct/icgc/indel/ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20161006.somatic.indel.vcf.gz","title":"Run VEP in offline mode"},{"location":"containers/ensembl-vep/#how-to-run-vcf2maf","text":"If you are running vcf2maf, it needs input vcf in uncompressed format. vcf2maf will internally run vep , so you may not need to run vep separately. Review how-to-run details at mskcc/vcf2maf and by reading manpage. You may endup getting incorrectly parsed maf if certain parameters are not specified per requirement, e.g., --tumor-id and --normal-id requirements for vcfs with tumor, normal samples. 1 singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl --help vcf2maf requires several dependencies. If you get an error, check out this detailed guide . If error persists, please search and/or submit an issue at mskcc/vcf2maf . 1 2 3 4 5 6 VEP_DIR = /opt/vep/src/ensembl-vep VEP_DATA = \" ${ RVANNOT } \" /vep_core/v99 REF_FASTA = /projects/verhaak-lab/hg19broad/Homo_sapiens_assembly19.fasta EXAC_VCF = /projects/verhaak-lab/verhaak_env/core_annots/exac/ExAC_nonTCGA.r0.3.1.sites.vep.vcf.gz export VEP_DIR VEP_DATA REF_FASTA EXAC_VCF In contrast to singularity vep image, note the use of singularity exec and not singularity run command for singularity vcf2maf image. Note that this example does NOT use --tumor-id and --normal-id parameters as vcf does not have those columns. However, this may not be a case for vcf output from common somatic callers, like mutect2, varscan2, etc. Increasing vcf-forks more than 4 does not improve conversion exponentially, so there may not be need of increasing threads. 1 2 3 4 5 6 7 8 9 10 singularity exec \" ${ SINGULARITY_SIF } \" /vcf2maf_v1.0_a071af6.sif /opt/vcf2maf/vcf2maf.pl \\ --vep-path \" ${ VEP_DIR } \" \\ --vep-data \" ${ VEP_DATA } \" \\ --ref-fasta \" ${ REF_FASTA } \" \\ --filter-vcf \" ${ EXAC_VCF } \" \\ --vep-forks 4 \\ --species homo_sapiens \\ --ncbi-build GRCh37 \\ --input-vcf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.vcf \\ --output-maf ffe4bb51-e98a-41a7-a4e1-c3970386889c.consensus.20160830.somatic.snv_mnv.maf END","title":"How to run vcf2maf"},{"location":"slurm/S01_slurm101/","text":"Slurm cheatsheet","title":"Slurm 101"},{"location":"slurm/how_to_interactive_via_tmux/","text":"Each time you login to Sumner HPC, you will be redirected to one of two login nodes. If you are using tmux and running interactive job (using slurm) on a compute node, you may prefer to detach it, say during lunch or meeting times and like to re-attach afterwards from your local machine. If so, you do need to remember on which of two login nodes your tmux session was running, and login to that node and re-attach tmux session. This bash wrapper around tmux and slurm scheduler will (for the most part) do that job. Using tmux -V . I've tested it with v2.9+ Copy start_interactive from docs/confs/bin directory to ~/bin/start_interactive and chmod 755 start_interactive . If needed, change default resources and walltime for interactive job command where variable INTJOB is specified. This script will change move TMUX_TMPDIR from /tmp to ~/logs/tmux and bind tmux session with session ID specific to login-node, e.g., S1 and S2 for respectively, login-node1 and login-node2. From your local machine, place a bash wrapper like ~/bin/run_interactive_hpc with following code, and do chmod 755 ~/bin/run_interactive_hpc You may need to add/edit sumner profile in local ~/.ssh/config . Local ~/.ssh/config file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ## example config for sumner. Host sumner Hostname login.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no Host sumner1 Hostname sumner-log1.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no Host sumner2 Hostname sumner-log2.sumner.jax.org User foo IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Port 22 ServerAliveInterval 40 ServerAliveCountMax 10 StrictHostKeyChecking no 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash ## Run or attach to interactive session on one of sumner login nodes ssh sumner -t 'bash -l -c \"start_interactive\"' exitstat1 = $? if [[ \" $exitstat1 \" ! = 0 ]] ; then echo -e \"\\n#####\\nAttempt one more time to match login node and tmux session\\n#####\\n\" sleep 5 ## extract stderr for parsing remote hostname ssh sumner -t 'bash -l -c \"start_interactive\"' > | /tmp/sumner_interactive exitstat2 = $? if [[ \" $exitstat2 \" ! = 0 ]] ; then echo -e \"\\nForce ssh to login node matching tmux session.\" ## get login node where tmux is running TMUXHOST = \" $( grep -Eo \"S[12]{1}INT\" /tmp/sumner_interactive ) \" echo -e \"\\ntmux session ID is ${ TMUXHOST } \\nForce login to server where tmux is running.\" ## Using ~/.ssh/config profile for a specific login node if [[ \" ${ TMUXHOST } \" == \"S1INT\" ]] ; then ssh sumner1 -t 'bash -l -c \"start_interactive\"' elif [[ \" ${ TMUXHOST } \" == \"S2INT\" ]] ; then ssh sumner2 -t 'bash -l -c \"start_interactive\"' else echo -e \"\\nERROR: Invalid pattern for remote tmux session ID: ${ TMUXHOST } \\nIt should be either S1INT or S2INT.\\nCan not start_interactive session.\" > & 2 exit 1 fi fi fi ## END ## From your local machine, run ~/bin/run_interactive_hpc and see if it works! If not, feel free to raise question at user forum . Notes Avoid running start_interactive while you are on login node or spawning new interactive job inside a custom tmux session. Instead, prefer running ~/bin/run_interactive_hpc from your local machine . Wrapper allows running only one interactive job with login-node specific job name, and relies on login-node specific tmux session else it will fail to work. If you like to go back to detached tmux session from login node, you can instead use tmux new-session -A -s S1 (or S2 for login-node2) to attach to either running session or create a new one. Details here or checkout docs/confs/bin/tmx .","title":"Interactive job inside tmux"}]}